{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from os.path import basename\n",
    "import matplotlib.pylab as plt\n",
    "import fnmatch\n",
    "import cv2\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import History\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path of the files:\n",
    "path = \"/home/MariaNuila/*/*/*.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pictures = glob(path)\n",
    "\n",
    "#Separate images based on classification\n",
    "nonIDC =  fnmatch.filter(pictures, '*class0.png')\n",
    "IDC = fnmatch.filter(pictures, '*class1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 50, 50\n",
    "num_classes = 2\n",
    "input_shape = (50,50,3)\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and testing data set from pictures\n",
    "x = []\n",
    "y = [] #labels\n",
    "#Attempt to get better depiction of dataset by grabbing from beginning, middle, and end\n",
    "for pic in pictures[0:30000]:\n",
    "    image = cv2.imread(pic)\n",
    "    x.append(cv2.resize(image, (width, height), interpolation=cv2.INTER_CUBIC))\n",
    "    if pic in nonIDC:\n",
    "        y.append(0)\n",
    "    if pic in IDC:\n",
    "        y.append(1)\n",
    "        \n",
    "for pic in pictures[70000:100000]:\n",
    "    image = cv2.imread(pic)\n",
    "    x.append(cv2.resize(image, (width, height), interpolation=cv2.INTER_CUBIC))\n",
    "    if pic in nonIDC:\n",
    "        y.append(0)\n",
    "    if pic in IDC:\n",
    "        y.append(1)\n",
    "        \n",
    "for pic in pictures[140000:170000]:\n",
    "    image = cv2.imread(pic)\n",
    "    x.append(cv2.resize(image, (width, height), interpolation=cv2.INTER_CUBIC))\n",
    "    if pic in nonIDC:\n",
    "        y.append(0)\n",
    "    if pic in IDC:\n",
    "        y.append(1)\n",
    "        \n",
    "for pic in pictures[240000:270000]:\n",
    "    image = cv2.imread(pic)\n",
    "    x.append(cv2.resize(image, (width, height), interpolation=cv2.INTER_CUBIC))\n",
    "    if pic in nonIDC:\n",
    "        y.append(0)\n",
    "    if pic in IDC:\n",
    "        y.append(1)\n",
    "        \n",
    "#Place in dataframe:\n",
    "df = pd.DataFrame()\n",
    "df[\"images\"] = x\n",
    "df[\"labels\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "#Vectorize and normalize data before training split\n",
    "x =np.array(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x / 255.0\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "#Convert our training and testing data into np arrays:\n",
    "xtrain_npArray = np.array(X_train)\n",
    "xtest_npArray = np.array(X_test)\n",
    "ytrain_npArray = np.array(Y_train)\n",
    "ytest_npArray = np.array(Y_test)\n",
    "# convert class vectors to binary class matrices\n",
    "ytrain_npArray = keras.utils.to_categorical(ytrain_npArray, num_classes)\n",
    "ytest_npArray = keras.utils.to_categorical(ytest_npArray, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Data training information: \n",
    "batch_size = 128\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96000, 50, 50, 3)\n",
      "(24000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_npArray.shape)\n",
    "print(ytest_npArray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess images further to fine-tune transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with imbalanced class sizes below\n",
    "# Make Data 1D for compatability upsampling methods\n",
    "X_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\n",
    "X_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\n",
    "X_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\n",
    "X_testFlat = X_test.reshape(X_test.shape[0], X_testShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train Shape: ', (96000, 50, 50, 3))\n",
      "('X_test Shape: ', (24000, 50, 50, 3))\n",
      "('X_trainFlat Shape: ', (96000, 7500))\n",
      "('X_testFlat Shape: ', (24000, 7500))\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train Shape: \",X_train.shape)\n",
    "print(\"X_test Shape: \",X_test.shape)\n",
    "print(\"X_trainFlat Shape: \",X_trainFlat.shape)\n",
    "print(\"X_testFlat Shape: \",X_testFlat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use random under sampling to deal with imbalance of dataset\n",
    "rus = RandomUnderSampler(ratio='auto')\n",
    "X_trainRus, Y_trainRus = rus.fit_sample(X_trainFlat, Y_train)\n",
    "X_testRus, Y_testRus = rus.fit_sample(X_testFlat, Y_test)\n",
    "\n",
    "Y_trainRusCat = to_categorical(Y_trainRus, num_classes = 2)\n",
    "Y_testRusCat = to_categorical(Y_testRus, num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_trainRos Shape: ', (52096, 7500))\n",
      "('Y_trainRosHot Shape: ', (52096, 2))\n"
     ]
    }
   ],
   "source": [
    "#Check that our under sampling balances the dataset\n",
    "print(\"X_trainRos Shape: \",X_trainRus.shape)\n",
    "print(\"Y_trainRosHot Shape: \",Y_trainRusCat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize all our images to the correct input size:\n",
    "for i in range(len(X_trainRus)):\n",
    "    height, width, channels = 50,50,3\n",
    "    X_trainRusReshaped = X_trainRus.reshape(len(X_trainRus),height,width,channels)\n",
    "    \n",
    "for i in range(len(X_testRus)):\n",
    "    height, width, channels = 50,50,3\n",
    "    X_testRusReshaped = X_testRus.reshape(len(X_testRus),height,width,channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with VGG16 Keras - Fine Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 50, 50, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 50, 50, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 25, 25, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 25, 25, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our fine-tuned model sequential for easier processing\n",
    "fine_tuned = Sequential()\n",
    "#Add vgg layers to our model\n",
    "for layers in vgg16_model.layers:\n",
    "    fine_tuned.add(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapt the previous transfer learning and allow the last 3 layers to be trainable, in hope for better results\n",
    "for layers in fine_tuned.layers[:-3]:\n",
    "    layers.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now add our fully connected layer: (adding another dropout for overfitting)\n",
    "fine_tuned.add(Flatten())\n",
    "fine_tuned.add(Dense(64, activation='relu'))\n",
    "fine_tuned.add(Dropout(0.3))\n",
    "fine_tuned.add(Dense(64, activation='relu'))\n",
    "fine_tuned.add(Dropout(0.5))\n",
    "fine_tuned.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 50, 50, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 50, 50, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 25, 25, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 25, 25, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 14,751,810\n",
      "Trainable params: 4,756,738\n",
      "Non-trainable params: 9,995,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fine_tuned.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Adam optimizer based on stochastic optimization (Adam):\n",
    "fine_tuned.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.00146),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96000 samples, validate on 24000 samples\n",
      "Epoch 1/15\n",
      "96000/96000 [==============================] - 975s 10ms/step - loss: 0.5847 - acc: 0.7287 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "Epoch 2/15\n",
      "96000/96000 [==============================] - 978s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "Epoch 3/15\n",
      "96000/96000 [==============================] - 974s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5807 - val_acc: 0.7327\n",
      "Epoch 4/15\n",
      "96000/96000 [==============================] - 972s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "Epoch 5/15\n",
      "96000/96000 [==============================] - 974s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5807 - val_acc: 0.7327\n",
      "Epoch 6/15\n",
      "96000/96000 [==============================] - 973s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5807 - val_acc: 0.7327\n",
      "Epoch 7/15\n",
      "96000/96000 [==============================] - 970s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "Epoch 8/15\n",
      "96000/96000 [==============================] - 973s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "Epoch 9/15\n",
      "96000/96000 [==============================] - 972s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "Epoch 10/15\n",
      "96000/96000 [==============================] - 973s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "Epoch 11/15\n",
      "96000/96000 [==============================] - 974s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "Epoch 12/15\n",
      "96000/96000 [==============================] - 977s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "Epoch 13/15\n",
      "96000/96000 [==============================] - 975s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "Epoch 14/15\n",
      "96000/96000 [==============================] - 975s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5807 - val_acc: 0.7327\n",
      "Epoch 15/15\n",
      "96000/96000 [==============================] - 975s 10ms/step - loss: 0.5846 - acc: 0.7287 - val_loss: 0.5807 - val_acc: 0.7327\n"
     ]
    }
   ],
   "source": [
    "history = fine_tuned.fit(xtrain_npArray, ytrain_npArray,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(xtest_npArray, ytest_npArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test loss:', 0.8075543790521421)\n",
      "('Test accuracy:', 0.5)\n"
     ]
    }
   ],
   "source": [
    "score = fine_tuned.evaluate(X_testRusReshaped, Y_testRusCat, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(history.history[\"loss\"],'r-x', label=\"Train Loss\")\n",
    "    ax.plot(history.history[\"val_loss\"],'b-x', label=\"Validation Loss\")\n",
    "    ax.legend()\n",
    "    ax.set_title('cross_entropy loss')\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(history.history[\"acc\"],'r-x', label=\"Train Accuracy\")\n",
    "    ax.plot(history.history[\"val_acc\"],'b-x', label=\"Validation Accuracy\")\n",
    "    ax.legend()\n",
    "    ax.set_title('accuracy')\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict our \"fine-tuned\" vgg16 keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fine_tuned.predict(X_testRusReshaped)\n",
    "map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n', classification_report(np.where(Y_testRusCat> 0)[1], np.argmax(y_pred, axis=1),target_names=list(map_characters.values())), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_classes = np.argmax(y_pred,axis=1) \n",
    "Y_true = np.argmax(Y_testRusCat,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "plot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values())) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
